{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with TensorFlow's `Dataset` API (continuation)\n",
    "\n",
    "In this notebook we will learn how to divide the dataset over the ranks in distributed training.\n",
    "\n",
    "The following steps were done on one of the previous notebooks. If necessary they can be run again on a new cell.\n",
    "```bash\n",
    "wget https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.csv\n",
    "echo \"sepal_length,sepal_width,petal_length,petal_width,species\" > iris_setosa.csv\n",
    "grep setosa iris.csv >> iris_setosa.csv\n",
    "echo \"sepal_length,sepal_width,petal_length,petal_width,species\" > iris_versic.csv\n",
    "grep versicolor iris.csv >> iris_versic.csv\n",
    "echo \"sepal_length,sepal_width,petal_length,petal_width,species\" > iris_virgin.csv\n",
    "grep virginica iris.csv >> iris_virgin.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.12.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_columns(*row, classes):\n",
    "    \"\"\"Convert the string classes to one-hot encoded:\n",
    "    setosa     -> [1, 0, 0]\n",
    "    virginica  -> [0, 1, 0]\n",
    "    versicolor -> [0, 0, 1]\n",
    "    \"\"\"\n",
    "    features = tf.convert_to_tensor(row[:4])\n",
    "    label_int = tf.where(tf.equal(classes, row[4]))\n",
    "    label = tf.one_hot(label_int, 3)\n",
    "    return features, label\n",
    "\n",
    "\n",
    "def get_csv_dataset(filename):\n",
    "    return tf.data.experimental.CsvDataset(filename, header=True,\n",
    "                                           record_defaults=[tf.float32,\n",
    "                                                            tf.float32,\n",
    "                                                            tf.float32,\n",
    "                                                            tf.float32,\n",
    "                                                            tf.string])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Shards <a id='using_shards'></a>\n",
    "\n",
    "\n",
    "Let's simulate a distributed training with two ranks to see what happens with the data on each worker. In distributed training one can use [`tf.data.Dataset.shard`]( https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shard) to divide the dataset over the ranks, otherwise the same data might be sent to each of the workers.\n",
    "\n",
    "> Note: `tf.data.Dataset.shard` is deprecated from TensorFlow-1.13, but it will replaced for another function which works very similar.\n",
    "\n",
    "Let's consider:\n",
    " * `tf.data.Dataset.list_files` with `shuffle=True`.\n",
    " * `tf.data.Dataset.list_files` with `shuffle=False`.\n",
    " * Shard before interleaving.\n",
    " * Shard after interleaving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common for both ranks\n",
    "dataset = tf.data.Dataset.list_files(['iris_setosa.csv',\n",
    "                                       'iris_versic.csv'],\n",
    "                                      shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: [5.1 3.5 1.4 0.2]  |  label: [[[1. 0. 0.]]]\n",
      "features: [4.9 3.  1.4 0.2]  |  label: [[[1. 0. 0.]]]\n",
      "features: [4.7 3.2 1.3 0.2]  |  label: [[[1. 0. 0.]]]\n",
      "features: [4.6 3.1 1.5 0.2]  |  label: [[[1. 0. 0.]]]\n",
      "features: [5.  3.6 1.4 0.2]  |  label: [[[1. 0. 0.]]]\n",
      "features: [5.4 3.9 1.7 0.4]  |  label: [[[1. 0. 0.]]]\n",
      "features: [4.6 3.4 1.4 0.3]  |  label: [[[1. 0. 0.]]]\n",
      "features: [5.  3.4 1.5 0.2]  |  label: [[[1. 0. 0.]]]\n",
      "features: [4.4 2.9 1.4 0.2]  |  label: [[[1. 0. 0.]]]\n",
      "features: [4.9 3.1 1.5 0.1]  |  label: [[[1. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# Rank 0\n",
    "dataset0 = dataset.shard(2, 0)  # hvd.size(), hvd.rank()\n",
    "dataset0 = dataset0.interleave(get_csv_dataset,\n",
    "                               cycle_length=2,\n",
    "                               block_length=1,\n",
    "                               num_parallel_calls=2)\n",
    "dataset0 = dataset0.map(lambda *row: parse_columns(*row, classes=['setosa', 'virginica', 'versicolor']))\n",
    "iterator0 = dataset0.make_one_shot_iterator()\n",
    "next_item0 = iterator0.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    try:\n",
    "        for i in range(10):\n",
    "            features, label = sess.run(next_item0)\n",
    "            print('features: %s  |  label: %s' % (features, label))\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print('The dataset ran out of entries!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: [7.  3.2 4.7 1.4]  |  label: [[[0. 0. 1.]]]\n",
      "features: [6.4 3.2 4.5 1.5]  |  label: [[[0. 0. 1.]]]\n",
      "features: [6.9 3.1 4.9 1.5]  |  label: [[[0. 0. 1.]]]\n",
      "features: [5.5 2.3 4.  1.3]  |  label: [[[0. 0. 1.]]]\n",
      "features: [6.5 2.8 4.6 1.5]  |  label: [[[0. 0. 1.]]]\n",
      "features: [5.7 2.8 4.5 1.3]  |  label: [[[0. 0. 1.]]]\n",
      "features: [6.3 3.3 4.7 1.6]  |  label: [[[0. 0. 1.]]]\n",
      "features: [4.9 2.4 3.3 1. ]  |  label: [[[0. 0. 1.]]]\n",
      "features: [6.6 2.9 4.6 1.3]  |  label: [[[0. 0. 1.]]]\n",
      "features: [5.2 2.7 3.9 1.4]  |  label: [[[0. 0. 1.]]]\n"
     ]
    }
   ],
   "source": [
    "# Rank 1\n",
    "dataset1 = dataset.shard(2, 1)  # hvd.size(), hvd.rank()\n",
    "dataset1 = dataset1.interleave(get_csv_dataset,\n",
    "                               cycle_length=2,\n",
    "                               block_length=1,\n",
    "                               num_parallel_calls=2)\n",
    "dataset1 = dataset1.map(lambda *row: parse_columns(*row, classes=['setosa', 'virginica', 'versicolor']))\n",
    "iterator1 = dataset1.make_one_shot_iterator()\n",
    "next_item1 = iterator1.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    try:\n",
    "        for i in range(10):\n",
    "            features, label = sess.run(next_item1)\n",
    "            print('features: %s  |  label: %s' % (features, label))\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print('The dataset ran out of entries!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
